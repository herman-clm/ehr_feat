{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the selected final model to evaluate its performance on the \"test\" dataset (300 pts). It requires feat_transformer.py, which contains the encoded feat model for resistant hypertension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Read test dataset (300 random patients)\n",
    "targets = {\n",
    "            'htn_dx_ia':'Htndx',\n",
    "            'res_htn_dx_ia':'ResHtndx', \n",
    "            'htn_hypok_dx_ia':'HtnHypoKdx', \n",
    "            'HTN_heuristic':'HtnHeuri', \n",
    "            'res_HTN_heuristic':'ResHtnHeuri',\n",
    "            'hypoK_heuristic_v4':'HtnHypoKHeuri'\n",
    "            }\n",
    "drop_cols = ['UNI_ID'] + list(targets.keys())\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "            '../Dataset' + str(101) + '/' + 'ResHtndx' + '/' + 'ResHtndxATest.csv')\n",
    "# filter just to random patients\n",
    "rand_df_test = pd.read_csv(\n",
    "            '../Dataset101/redcap_data_access_group.csv')\n",
    "rand_ids_test = rand_df_test.loc[rand_df_test['redcap_data_access_group'] == 'pc_200','UNI_ID']\n",
    "# rand_ids = rand_df.loc[rand_df['redcap_data_access_group'] == 'reshtn_op_v3','UNI_ID']\n",
    "random_mask = df_test['UNI_ID'].isin(rand_ids_test)\n",
    "flagged_mask = ~df_test['UNI_ID'].isin(rand_ids_test)\n",
    "df_test_flagged = df_test.loc[flagged_mask]\n",
    "df_test = df_test.loc[random_mask]\n",
    "X_test = df_test.drop(drop_cols,axis=1)  \n",
    "y_test = df_test['res_htn_dx_ia'].values\n",
    "y_heu = df_test['res_HTN_heuristic']\n",
    "\n",
    "X_test_flagged = df_test_flagged.drop(drop_cols,axis=1)  \n",
    "y_test_flagged = df_test_flagged['res_htn_dx_ia'].values\n",
    "y_heu_flagged = df_test_flagged['res_HTN_heuristic']\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "            '../Dataset' + str(101) + '/' + 'ResHtndx' + '/' + 'ResHtndxATrain.csv')\n",
    "# filter training samples to random set\n",
    "rand_df_train = pd.read_csv(\n",
    "            '../Dataset101/redcap_data_access_group_train.csv')\n",
    "rand_ids_train = rand_df_train.loc[rand_df_train['redcap_data_access_group'] == 'pc_200','UNI_ID']\n",
    "df_train = df_train.loc[df_train['UNI_ID'].isin(rand_ids_train)]\n",
    "X_train = df_train.drop(drop_cols,axis=1)\n",
    "y_train = df_train['res_htn_dx_ia'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df_train['redcap_data_access_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training samples:',len(X_train),\n",
    "'disease prevalence:',np.sum(y_train==1)/len(y_train))\n",
    "print('test samples:',len(X_test),\n",
    "'disease prevalence:',np.sum(y_test==1)/len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from feat_transformer import FeatTransformer\n",
    "\n",
    "ft_lr_estimator = Pipeline( [\n",
    "    ('prep', FeatTransformer()),\n",
    "    ('est', LogisticRegression(C=1.0, penalty='l2', intercept_scaling=1.0, solver='liblinear'))\n",
    "]\n",
    ")\n",
    "\n",
    "ft_lr_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating everything on Training first:\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "\n",
    "y_pred_proba = ft_lr_estimator.predict_proba(X_train)[:,1]\n",
    "\n",
    "def prc_values(y,y_pred_proba):\n",
    "    precision, recall, prcthresholds = precision_recall_curve(y, y_pred_proba, pos_label=1)\n",
    "    max_prec = np.max(precision)\n",
    "    first = pd.DataFrame({\"precision\":[max_prec], \n",
    "                        \"recall\":[0],\n",
    "                        'thresholds':[0]})\n",
    "    prc = pd.DataFrame(list(zip(precision, recall, prcthresholds)), columns =['precision', 'recall', 'thresholds']) \n",
    "    prc = prc.append(first, sort=False)\n",
    "    prc = prc.sort_values(by='recall')\n",
    "    precision = prc['precision']\n",
    "    recall = prc['recall']\n",
    "    mean_recall = np.linspace(0, 1, 100)\n",
    "    precision = interp(mean_recall, recall, precision)\n",
    "    return mean_recall, precision, prc\n",
    "\n",
    "mean_recall, mean_precisions, prc_df = prc_values(y_train,y_pred_proba)\n",
    "\n",
    "#Back calculate confusion matrix to get sensitivity \n",
    "#tp = recall * total positives\n",
    "pos = np.sum(y_train)\n",
    "neg = len(y_train)-pos\n",
    "prc_df['tp'] = prc_df['recall']*pos\n",
    "prc_df['fn'] = pos-prc_df['tp']\n",
    "prc_df['fp'] = (prc_df['tp']-(prc_df['precision']*prc_df['tp']))/(prc_df['precision'])\n",
    "prc_df['tn'] = neg-prc_df['fp']\n",
    "prc_df['specificity'] = prc_df['tn']/(prc_df['tn']+prc_df['fp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_score(y_train,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's adjust by global prevalence of resistant hypertension (7.5%)\n",
    "\n",
    "prevs = [0.075]\n",
    "for prev in prevs:\n",
    "    prc_df['adj_precision'+str(prev)] = (prc_df['recall']*prev)/((prc_df['recall']*prev)+((1-prc_df['specificity'])*(1-prev)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a threshold\n",
    "\n",
    "We have a target precision of **0.7**. What probability threshold would we choose to hit this on our training set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PPV = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustment\n",
    "chosen_prc = prc_df.loc[prc_df['adj_precision0.075']>=TARGET_PPV].sort_values(by='adj_precision0.075').iloc[0]\n",
    "# without adjusting\n",
    "# chosen_prc = prc_df.loc[prc_df['precision']>=TARGET_PPV].sort_values(by='precision').iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chosen_Threshold = chosen_prc['thresholds']\n",
    "# Chosen_Threshold = 0.5\n",
    "print('Chosen Threshold:',Chosen_Threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use target precision of 0.7; our probability threshold based on the training set is 0.595807. This gives a precision recall pair of (0.8,0.621). (0.715 adjusted PPV, 0.64  recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feat Final Model Adjusted Precision/Recall Resistant Hypertension (random subset)\", \n",
    "          loc='left', fontsize=14)\n",
    "plt.plot(mean_recall, mean_precisions, ':b',alpha=0.75,label = \"Unadjusted\")\n",
    "for prev in prevs:\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.plot(prc_df['recall'], prc_df['adj_precision'+str(prev)], 'b', label = 'Prevalence = '+str(prev))\n",
    "    for i, (r, ap,thresh) in enumerate(\n",
    "        zip(prc_df['recall'], prc_df['adj_precision'+str(prev)], prc_df['thresholds'])):\n",
    "        if i % 10 == 0:\n",
    "            plt.text(r,ap+0.03,str(round(thresh,3)), rotation=30)\n",
    "    chosen_prc = prc_df.iloc[(prc_df['thresholds']-Chosen_Threshold).abs().argsort().values[0]]\n",
    "    plt.plot([chosen_prc['recall'],chosen_prc['recall']], \n",
    "             [0, chosen_prc['adj_precision'+str(prev)]], ':k', alpha=1)\n",
    "    plt.text(x= chosen_prc['recall'], \n",
    "             y=0.5*chosen_prc['adj_precision'+str(prev)],\n",
    "             s=\"Chosen Threshold = \"+str(round(Chosen_Threshold,3)) \n",
    "                +\"\\nRecall = \" +str(round(chosen_prc['recall'],3)))\n",
    "plt.plot([0,1],[TARGET_PPV,TARGET_PPV],'--r', label='Target precision')\n",
    "plt.legend()\n",
    "plt.savefig('figs/adjusted_AUPRC_training.pdf',dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat everything on the testing data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_proba = ft_lr_estimator.predict_proba(X_test)[:,1]\n",
    "\n",
    "mean_recall, mean_precisions, prc_df = prc_values(y_test,y_pred_proba)\n",
    "\n",
    "#Back calculate confusion matrix to get sensitivity \n",
    "#tp = recall * total positives\n",
    "pos = np.sum(y_test)\n",
    "neg = len(y_test)-pos\n",
    "prc_df['tp'] = prc_df['recall']*pos\n",
    "prc_df['fn'] = pos-prc_df['tp']\n",
    "prc_df['fp'] = (prc_df['tp']-(prc_df['precision']*prc_df['tp']))/(prc_df['precision'])\n",
    "prc_df['tn'] = neg-prc_df['fp']\n",
    "prc_df['specificity'] = prc_df['tn']/(prc_df['tn']+prc_df['fp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now for the heuristic-flagged patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_proba_flagged = ft_lr_estimator.predict_proba(X_test_flagged)[:,1]\n",
    "\n",
    "mean_recall_flagged, mean_precisions_flagged, prc_df_flagged = prc_values(y_test_flagged,y_pred_proba_flagged)\n",
    "\n",
    "#Back calculate confusion matrix to get sensitivity \n",
    "#tp = recall * total positives\n",
    "pos = np.sum(y_test)\n",
    "neg = len(y_test)-pos\n",
    "prc_df_flagged['tp'] = prc_df_flagged['recall']*pos\n",
    "prc_df_flagged['fn'] = pos-prc_df_flagged['tp']\n",
    "prc_df_flagged['fp'] = (prc_df_flagged['tp']-(prc_df_flagged['precision']*prc_df_flagged['tp']))/(prc_df_flagged['precision'])\n",
    "prc_df_flagged['tn'] = neg-prc_df_flagged['fp']\n",
    "prc_df_flagged['specificity'] = prc_df_flagged['tn']/(prc_df_flagged['tn']+prc_df_flagged['fp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prev in prevs:\n",
    "    prc_df['adj_precision'+str(prev)] = (prc_df['recall']*prev)/((prc_df['recall']*prev)+((1-prc_df['specificity'])*(1-prev)))\n",
    "    prc_df_flagged['adj_precision'+str(prev)] = (prc_df_flagged['recall']*prev)/(\n",
    "        (prc_df_flagged['recall']*prev)+((1-prc_df_flagged['specificity'])*(1-prev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine similar adjusted precision values at similar thresholds\n",
    "display(prc_df.iloc[(prc_df['thresholds']-Chosen_Threshold).argsort().values[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model with a probabliity threshold as determined in the training set would have adjusted PPV of 0.535 on general population at 7.5% prevalence. Compare to training (adjusted PPV 0.696, recall 0.64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat adjustment for heuristic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "        \n",
    "heu = {\n",
    "       'average_precision_score_test': average_precision_score(y_test, y_heu),\n",
    "       'precision': np.sum((y_heu==1) & (y_test == 1))/ np.sum(y_heu==1),\n",
    "       'recall': np.sum((y_heu==1) & (y_test == 1))/ np.sum(y_test==1),\n",
    "       'specificity': np.sum((y_heu==0) & (y_test == 0))/ np.sum(y_test==0),\n",
    "       'roc_auc_score_test': roc_auc_score(y_test, y_heu)\n",
    "      }\n",
    "heu_recall = heu['recall']\n",
    "heu_spec = heu['specificity']\n",
    "for prev in prevs:\n",
    "    heu_ppv_adj = heu_recall * prev / (heu_recall*prev + (1-heu_spec)*(1-prev))\n",
    "    print('Heuristic Adjusted PPV for prevalence=',prev*100,'%:',heu_ppv_adj)\n",
    "print('Heuristc performance:',heu)\n",
    "print('Heuristic confusion matrix')\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_heu)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare test set FEAT model to heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feat Final Model Adjusted Precision/Recall Resistant Hypertension \", \n",
    "          loc='left', fontsize=14)\n",
    "plt.plot(mean_recall, mean_precisions, ':b',alpha=0.75,label = \"Unadjusted\")\n",
    "for prev in prevs:\n",
    "    plt.title(\"Feat Final Model, Random Test Patients, Resistant Hypertension (Prevalence =\"\n",
    "#     plt.title(\"Feat Final Model, Random Test Patients, Resistant Hypertension (Prevalence =\"\n",
    "              +str(prev)+\")\", loc='left', fontsize=14)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.plot(prc_df['recall'], prc_df['adj_precision'+str(prev)], '-bx',\n",
    "             label = 'Prevalence = '+str(prev))\n",
    "    chosen_prc = prc_df.loc[prc_df['thresholds']>=Chosen_Threshold].sort_values(by='thresholds').iloc[0]\n",
    "#     print('prc_df thresholds:',prc_df['thresholds'])\n",
    "    print(chosen_prc)\n",
    "    plt.plot([chosen_prc['recall'],chosen_prc['recall']], \n",
    "             [0, 1], ':k', alpha=1)\n",
    "    plt.text(x= chosen_prc['recall']+0.01, \n",
    "             y=0.3*chosen_prc['adj_precision'+str(prev)],\n",
    "#              y=chosen_prc['adj_precision'+str(prev)]+0.2,\n",
    "             s=\"Chosen Threshold = \"+str(round(Chosen_Threshold,3))\n",
    "               +\"\\nPPV = \"+str(round(chosen_prc['adj_precision'+str(prev)],4))\n",
    "               +\"\\nRecall = \"+str(round(chosen_prc['recall'],4)))\n",
    "    plt.plot(chosen_prc['recall'], chosen_prc['adj_precision'+str(prev)], 'Xk')\n",
    "\n",
    "plt.plot(heu_recall, heu_ppv_adj, 'sr', label='Heuristic')\n",
    "plt.text(heu_recall-0.01, heu_ppv_adj,\n",
    "         s= 'PPV = '+str(round(heu_ppv_adj,2))+'\\nRecall = '+str(round(heu_recall,2)),\n",
    "        label='Heuristic',horizontalalignment='right')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('figs/adjusted_AUPRC_test_random_pts_thresh_random.pdf',dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison on flagged patients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adjusted heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "        \n",
    "heu_flagged = {\n",
    "       'average_precision_score_test': average_precision_score(y_test_flagged, y_heu_flagged),\n",
    "       'precision': np.sum((y_heu_flagged==1) & (y_test_flagged == 1))/ np.sum(y_heu_flagged==1),\n",
    "       'recall': np.sum((y_heu_flagged==1) & (y_test_flagged == 1))/ np.sum(y_test_flagged==1),\n",
    "       'specificity': np.sum((y_heu_flagged==0) & (y_test_flagged == 0))/ np.sum(y_test_flagged==0),\n",
    "       'roc_auc_score_test': roc_auc_score(y_test_flagged, y_heu_flagged)\n",
    "      }\n",
    "heu_recall_flagged = heu_flagged['recall']\n",
    "heu_spec_flagged = heu_flagged['specificity']\n",
    "for prev in prevs:\n",
    "    heu_ppv_adj_flagged = heu_recall_flagged * prev / (heu_recall_flagged*prev + (1-heu_spec_flagged)*(1-prev))\n",
    "    print('Heuristic Adjusted PPV for prevalence=',prev*100,'%:',heu_ppv_adj_flagged)\n",
    "print('Heuristc performance:',heu_flagged)\n",
    "print('Heuristic confusion matrix')\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test_flagged, y_heu_flagged)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feat Final Model Adjusted Precision/Recall Resistant Hypertension \", \n",
    "          loc='left', fontsize=14)\n",
    "# plt.plot(mean_recall_flagged, mean_precisions_flagged, ':b',alpha=0.75,label = \"Unadjusted\")\n",
    "for prev in prevs:\n",
    "    plt.title(\"Feat Final Model, Flagged Test Patients, Resistant Hypertension (Prevalence =\"\n",
    "#     plt.title(\"Feat Final Model, Random Test Patients, Resistant Hypertension (Prevalence =\"\n",
    "              +str(prev)+\")\", loc='left', fontsize=14)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.plot(prc_df_flagged['recall'], prc_df_flagged['adj_precision'+str(prev)], '-bx',\n",
    "             label = 'Prevalence = '+str(prev))\n",
    "    #TODO: pick closest, but above or equal to threshold\n",
    "#     chosen_prc = prc_df_flagged.iloc[(prc_df_flagged['thresholds']-Chosen_Threshold).abs().argsort().values[0]]\n",
    "    chosen_prc = prc_df_flagged.iloc[(prc_df_flagged['thresholds']-Chosen_Threshold).abs().argsort().values[0]]\n",
    "    \n",
    "    print('prc_df thresholds:',prc_df_flagged['thresholds'])\n",
    "    print(chosen_prc)\n",
    "    plt.plot([chosen_prc['recall'],chosen_prc['recall']], \n",
    "             [0, 1], ':k', alpha=1)\n",
    "    plt.text(x= chosen_prc['recall']+0.01, \n",
    "             y=0.3*chosen_prc['adj_precision'+str(prev)],\n",
    "#              y=chosen_prc['adj_precision'+str(prev)]+0.2,\n",
    "             s=\"Chosen Threshold = \"+str(round(Chosen_Threshold,3))\n",
    "               +\"\\nPPV = \"+str(round(chosen_prc['adj_precision'+str(prev)],2))\n",
    "               +\"\\nRecall = \"+str(round(chosen_prc['recall'],2)))\n",
    "    plt.plot(chosen_prc['recall'], chosen_prc['adj_precision'+str(prev)], 'Xk')\n",
    "\n",
    "plt.plot(heu_flagged['recall'], heu_flagged['precision'], 'sr', label='Heuristic')\n",
    "\n",
    "plt.text(heu_flagged['recall']-0.01, heu_flagged['precision'],\n",
    "         s= 'PPV = '+str(round(heu_flagged['precision'],2))\n",
    "         +'\\nRecall = '+str(round(heu_flagged['recall'],2)),\n",
    "        label='Heuristic',horizontalalignment='right')\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "plt.savefig('figs/adjusted_AUPRC_test_flagged_pts_thresh_random.pdf',dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
