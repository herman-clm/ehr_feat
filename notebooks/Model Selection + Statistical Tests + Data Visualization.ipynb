{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook 1) aggregates and averages the five-fold results and 2) implements the model selection procedure based on the repeated cross-validation results. Final outputs are saved as \"select_perf.csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#embed fonts\n",
    "import matplotlib\n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "import scipy.stats\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdir = 'results_2020-09-14/'\n",
    "long_vars = ['pred','pred_proba']\n",
    "frames = []\n",
    "frames_long = []\n",
    "feat_frames = []\n",
    "feat_frames_long = []\n",
    "#Read benchmark model results\n",
    "for file in tqdm(glob.glob('../'+rdir+'/*/*/*.json')):\n",
    "    with open(file,'r') as of:\n",
    "        results = json.load(of)\n",
    "    if 'Feat' in file:\n",
    "        feat_frames.append(results)\n",
    "    else:\n",
    "        frames.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(frames)\n",
    "feat_df_results = pd.DataFrame.from_records(feat_frames)\n",
    "print('df_results columns:',df_results.columns)    \n",
    "print('models:',df_results.model.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df_results.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## down-select FEAT models from runs using heuristic procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import select_feat_models, smallest_of_best_three_quartiles\n",
    "\n",
    "feat_df_results_reduced = select_feat_models(feat_df_results, method= smallest_of_best_three_quartiles)\n",
    "# feat_df_results_reduced = select_feat_models(feat_df_results, method= smallest_of_best_half)\n",
    "# feat_df_results_reduced['selection'] = 'smallest_of_best_half' \n",
    "# feat_df_results_reduced = select_feat_models(feat_df_results, method= best_of_smallest_half)\n",
    "# feat_df_results_reduced2['selection'] = 'best_of_smallest_half' \n",
    "# feat_df_results_reduced3 = select_feat_models(feat_df_results, method= best)\n",
    "# feat_df_results_reduced3['selection'] = 'best' \n",
    "# feat_df_results_reduced4 = select_feat_models(feat_df_results, method= smallest)\n",
    "# feat_df_results_reduced4['selection'] = 'smallest' \n",
    "\n",
    "# feat_df_results_reduced = select_feat_models(feat_df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df_results_reduced.target.unique()\n",
    "# feat_df_results.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dataframes\n",
    "df_results = df_results.append(feat_df_results_reduced)\n",
    "# df_results = df_results.append(feat_df_results_reduced2)\n",
    "# df_results = df_results.append(feat_df_results_reduced3)\n",
    "# df_results = df_results.append(feat_df_results_reduced4)\n",
    "# df_results_long.append(feat_df_results_long_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.isna().any()\n",
    "df_results.model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make nice labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['RandomForest',\n",
    "          'DecisionTree',\n",
    "          'Feat_boolean',\n",
    "          'Feat_boolean_L1',\n",
    "          'GaussianNaiveBayes',\n",
    "          'LogisticRegression_L2',\n",
    "          'LogisticRegression_L1']\n",
    "model_nice = ['RF',\n",
    "          'DT',\n",
    "          'FEAT',\n",
    "          'FEAT L1',\n",
    "          'GNB',\n",
    "          'LR L2',\n",
    "          'LR L1']\n",
    "nice_model_labels = {k:v for k,v in zip(models,model_nice)}\n",
    "df_results['model_nice'] = df_results['model'].apply(lambda x: nice_model_labels[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dnames = ['HtnHeuri','HtnHypoKHeuri',\"ResHtnHeuri\", 'Htndx',\"HtnHypoKdx\",\"ResHtndx\"]\n",
    "dnames_nice = [ 'HTN Heuristic','HTN-Hypokalemia Heuristic',\"Resistant HTN Heuristic\",\n",
    "                'HTN Diagnosis',\"HTN-Hypokalemia Diagnosis\",\"Resistant HTN Diagnosis\"]\n",
    "dnames_to_nice = {k:v for k,v in zip(dnames, dnames_nice)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save output to select_perf.csv\n",
    "import os\n",
    "if not os.path.exists(rdir):\n",
    "    os.mkdir(rdir)\n",
    "df_results.to_csv(rdir + '/select_perf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average values by run (average all folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['size'] = df_results['size'].astype(float)\n",
    "df_results_ave = df_results.groupby(['model_nice','target','RunID'], as_index=False).mean()\n",
    "# df_results_ave = df_results.groupby(['model_nice','target','RunID','selection'], as_index=False).mean()\n",
    "df_results_ave.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make table in docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "\n",
    "document = Document()\n",
    "\n",
    "table = document.add_table(rows=1, cols=5)\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'Phenotype'\n",
    "hdr_cells[1].text = 'Method'\n",
    "hdr_cells[2].text = 'Median CV AUPRC (IQR)'\n",
    "hdr_cells[3].text = 'Median CV AUROC (IQR)'\n",
    "hdr_cells[4].text = 'Median Size (IQR)'\n",
    "# for target, dft in df_results_ave.groupby('target'):\n",
    "i = 0\n",
    "for target in dnames:\n",
    "    dft = df_results_ave.loc[df_results_ave.target == target]\n",
    "    j = 0\n",
    "    for model in ['GNB','DT','LR L1','LR L2','RF','FEAT']:\n",
    "        dftm = dft.loc[dft.model_nice == model]\n",
    "        i += 1\n",
    "        j += 1\n",
    "        table.add_row()\n",
    "        cells = table.rows[i].cells\n",
    "        if j == 4:\n",
    "            cells[0].text = dnames_to_nice[target]\n",
    "        cells[1].text = model\n",
    "        cells[2].text = '{:0.2f} (+-{:0.2f})'.format(\n",
    "            dftm['average_precision_score_test'].median(),\n",
    "            dftm['average_precision_score_test'].quantile(0.75) \n",
    "            - dftm['average_precision_score_test'].quantile(0.25)\n",
    "        )\n",
    "        cells[3].text = '{:0.2f} (+-{:0.2f})'.format(\n",
    "            dftm['roc_auc_score_test'].median(),\n",
    "            dftm['roc_auc_score_test'].quantile(0.75) \n",
    "            - dftm['roc_auc_score_test'].quantile(0.25)\n",
    "            )\n",
    "        cells[4].text = '{:0.2f} (+-{:0.2f})'.format(\n",
    "            dftm['size'].median(),\n",
    "            dftm['size'].quantile(0.75) \n",
    "            - dftm['size'].quantile(0.25)\n",
    "            )\n",
    "        \n",
    "# for qty, id, desc in records:\n",
    "#     row_cells = table.add_row().cells\n",
    "#     row_cells[0].text = str(qty)\n",
    "#     row_cells[1].text = id\n",
    "#     row_cells[2].text = desc\n",
    "\n",
    "document.add_page_break()\n",
    "if not os.path.exists('tables'):\n",
    "    os.mkdir('tables')\n",
    "document.save('tables/Table_1_model_performance.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_ave.model_nice.unique()\n",
    "df_results_ave[df_results_ave.model_nice=='FEAT'].target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "stat_to_nice = {\n",
    "    'average_precision_score_train':'AUPRC, Train',\n",
    "    'average_precision_score_test':'AURPRC, Test',\n",
    "    'size':'Model Size'\n",
    "}\n",
    "\n",
    "def make_boxplot(df, stats, targets=None, order=None, hue=None, hue_order=None, name=None):\n",
    "    if type(stats) is not list:\n",
    "        stats = [stats]\n",
    "    if targets == None:\n",
    "        targets = dnames\n",
    "        nrows = 2\n",
    "        ncols = 3\n",
    "    else:\n",
    "        nrows = len(stats)\n",
    "        ncols = len(targets)\n",
    "#         ncols = math.ceil(len(targets)/nrows)\n",
    "    if order == None: \n",
    "        order = ['GNB','DT', 'LR L2','LR L1','RF','FEAT']\n",
    "    if hue == None:\n",
    "        color = 'w'\n",
    "        palette = None\n",
    "    else:\n",
    "        color = None\n",
    "        palette = 'colorblind'\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    fig = plt.figure(figsize=(5*ncols,4*nrows))\n",
    "\n",
    "    i = 1\n",
    "    for stat in stats:\n",
    "        for target in targets:\n",
    "            df = df_results_ave[df_results_ave.target==target]\n",
    "            ax = fig.add_subplot(nrows,ncols,i)\n",
    "            sns.boxplot(x=\"model_nice\", \n",
    "                        y=stat, \n",
    "                        data=df, \n",
    "                        color = color,\n",
    "                        ax=ax, \n",
    "                        order = order,\n",
    "                        notch = True,\n",
    "                        hue=hue,\n",
    "                        hue_order=hue_order,\n",
    "                        palette=palette\n",
    "                       )\n",
    "            # make box edges black\n",
    "            for j, box in enumerate(ax.artists):\n",
    "                box.set_edgecolor('black')\n",
    "                # iterate over whiskers and median lines\n",
    "                for k in range(6*j,6*(j+1)):\n",
    "                     ax.lines[k].set_color('black') \n",
    "\n",
    "            # TODO: \n",
    "                # rename titles of datasets\n",
    "                # same y scaling?\n",
    "                # smaller names of models?\n",
    "            if i != ncols * nrows:\n",
    "                try:\n",
    "                    ax.legend_.remove()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            if i in [1,1+ncols]:\n",
    "                if stat in stat_to_nice.keys():\n",
    "                    plt.ylabel(stat_to_nice[stat])\n",
    "            else: \n",
    "                plt.ylabel('')\n",
    "            if nrows>1 and i <= ncols:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                ax.set_xticklabels(ax.xaxis.get_majorticklabels(),rotation=0)\n",
    "            plt.xlabel('')\n",
    "            if stat == 'size':\n",
    "                plt.gca().set_yscale('log')\n",
    "        #     plt.ylim(0.3,1.05)\n",
    "            plt.title(dnames_to_nice[target])\n",
    "            i += 1\n",
    "    # save figure\n",
    "    plt.tight_layout()\n",
    "    if not os.path.exists('figs/'+rdir):\n",
    "        os.mkdir('figs/'+rdir)\n",
    "    for filetype in ['.png','.pdf','.eps','.svg']:\n",
    "        if name != None:\n",
    "            plt.savefig('figs/'+rdir+'/'+name+filetype, dpi=400)\n",
    "        else:\n",
    "            plt.savefig('figs/'+rdir+'/boxplot_'+'_'.join(targets+stats)+filetype,dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue_order = ['best','smallest_of_best_half','best_of_smallest_half','smallest']\n",
    "# make_boxplot(df_results_ave.loc[df_results_ave.model_nice == 'FEAT'], 'average_precision_score_test',\n",
    "#              order=['FEAT'], hue='selection', \n",
    "#              hue_order = hue_order, \n",
    "#              name = 'boxplot_FEAT_selection_comparison_aps')\n",
    "# make_boxplot(df_results_ave.loc[df_results_ave.model_nice == 'FEAT'], 'size', \n",
    "#              order=['FEAT'], hue='selection',\n",
    "#              hue_order=hue_order,\n",
    "#              name = 'boxplot_FEAT_selection_comparison_size'\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = df_results_ave\n",
    "# df_to_plot = df_results_ave.loc[~df_results_ave.selection.isin(['best','smallest','best_of_smallest_half'])]\n",
    "make_boxplot(df_to_plot, 'average_precision_score_train')\n",
    "make_boxplot(df_to_plot, 'average_precision_score_test')\n",
    "make_boxplot(df_to_plot, 'size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make specific subplot boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_boxplot(df_results, 'average_precision_score_test', targets = dnames[:3])\n",
    "make_boxplot(df_results, 'size', targets = dnames[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_boxplot(df_results, ['average_precision_score_test','size'], targets = dnames[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_to_nice = {\n",
    "    'average_precision_score_train':'AUPRC, Train',\n",
    "    'average_precision_score_test':'AUPRC, Test',\n",
    "    'size':'Model Size'\n",
    "}\n",
    "order = ['GNB','DT', 'LR L2','LR L1','RF','FEAT']\n",
    "sns.set_style('whitegrid')\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "markers = ('s','^','h','p','D','o')\n",
    "marker_choice = {\n",
    "    'GNB':'s',\n",
    "    'DT':'^',\n",
    "    'LR L2':'h',\n",
    "    'LR L1':'p',\n",
    "    'RF':'D',\n",
    "    'FEAT':'o'\n",
    "}\n",
    "i = 1\n",
    "for target in dnames:\n",
    "    df = df_results_ave[df_results_ave.target==target]\n",
    "    print('df models:',df.model_nice.unique())\n",
    "    df = df[df.model_nice.isin(order)]\n",
    "    print('df models order filter:',df.model_nice.unique())\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,i)\n",
    "    \n",
    "    g = sns.scatterplot(data = df,\n",
    "                    x = 'size', \n",
    "                    y = 'average_precision_score_test',\n",
    "                    hue='model_nice',\n",
    "                    hue_order=order,\n",
    "                    palette='colorblind',\n",
    "#                     marker=marker_choice[df['model_nice']],\n",
    "                    style = 'model_nice',\n",
    "#                     markers = ('^','o', 's', 'p', 'h', 'D', 'P', 'X', '*','v', '<', '>',),\n",
    "#                     markers = list(marker_choice.values())\n",
    "#                     markers=markers\n",
    "                   )\n",
    "    plt.gca().set_xscale('log')\n",
    "    if i != 6:\n",
    "        plt.gca().legend_.remove()\n",
    "    else:\n",
    "        handles, _ = ax.get_legend_handles_labels()\n",
    "        plt.legend(handles,order)\n",
    "\n",
    "    if i in [1,4]:\n",
    "        plt.ylabel(stat_to_nice['average_precision_score_test'])\n",
    "    else: \n",
    "        plt.ylabel('')\n",
    "    if i > 3:\n",
    "        plt.xlabel(stat_to_nice['size'])\n",
    "    else: \n",
    "        plt.xlabel('')\n",
    "    plt.title(dnames_to_nice[target])\n",
    "    i += 1\n",
    "# save figure\n",
    "plt.tight_layout()\n",
    "if not os.path.exists('figs/'+rdir):\n",
    "    os.mkdir('figs/'+rdir)\n",
    "for filetype in ['.png','.pdf','.eps','.svg']:\n",
    "    plt.savefig('figs/'+rdir+'/pareto_plot_all'+filetype,dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pareto tradeoffs, both objectives minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_to_nice = {\n",
    "    'average_precision_score_train':'AUPRC, Train',\n",
    "    'average_precision_score_test':'AUPRC, Test',\n",
    "    'size':'Model Size'\n",
    "}\n",
    "order = ['GNB','DT', 'LR L2','LR L1','RF','FEAT']\n",
    "sns.set_style('whitegrid')\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "markers = ('s','^','h','p','D','o')\n",
    "marker_choice = {\n",
    "    'GNB':'s',\n",
    "    'DT':'^',\n",
    "    'LR L2':'h',\n",
    "    'LR L1':'p',\n",
    "    'RF':'D',\n",
    "    'FEAT':'o'\n",
    "}\n",
    "i = 1\n",
    "for target in dnames:\n",
    "    df = df_results_ave[df_results_ave.target==target]\n",
    "    print('df models:',df.model_nice.unique())\n",
    "    df = df[df.model_nice.isin(order)]\n",
    "    print('df models order filter:',df.model_nice.unique())\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,i)\n",
    "    \n",
    "    df['neg_average_precision_score_test'] = 1 - df['average_precision_score_test']\n",
    "    g = sns.scatterplot(data = df,\n",
    "                    x = 'size', \n",
    "                    y = 'neg_average_precision_score_test',\n",
    "                    hue='model_nice',\n",
    "                    hue_order=order,\n",
    "                    palette='colorblind',\n",
    "#                     marker=marker_choice[df['model_nice']],\n",
    "                    style = 'model_nice',\n",
    "#                     markers = ('^','o', 's', 'p', 'h', 'D', 'P', 'X', '*','v', '<', '>',),\n",
    "#                     markers = list(marker_choice.values())\n",
    "#                     markers=markers\n",
    "                   )\n",
    "    plt.gca().set_xscale('log')\n",
    "    if i != 6:\n",
    "        plt.gca().legend_.remove()\n",
    "    else:\n",
    "        handles, _ = ax.get_legend_handles_labels()\n",
    "        plt.legend(handles,order)\n",
    "\n",
    "    if i in [1,4]:\n",
    "        plt.ylabel('1 - ' +stat_to_nice['average_precision_score_test'])\n",
    "    else: \n",
    "        plt.ylabel('')\n",
    "    if i > 3:\n",
    "        plt.xlabel(stat_to_nice['size'])\n",
    "    else: \n",
    "        plt.xlabel('')\n",
    "    plt.title(dnames_to_nice[target])\n",
    "    i += 1\n",
    "# save figure\n",
    "plt.tight_layout()\n",
    "if not os.path.exists('figs/'+rdir):\n",
    "    os.mkdir('figs/'+rdir)\n",
    "for filetype in ['.png','.pdf','.eps','.svg']:\n",
    "    plt.savefig('figs/'+rdir+'/pareto_plot_all_minimized'+filetype,dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical test for AUPRCs modeling Heuristics (Wilcoxon rank-sum)\n",
    "from scipy.stats import wilcoxon\n",
    "from statannot import add_stat_annotation\n",
    "import pdb\n",
    "sns.set_style('whitegrid')\n",
    "def stats_test_box(df, datasetses, comparisons):\n",
    "    \"\"\"Compare results from df1 and df2 over models on datasets according to comparison.\n",
    "        df1 should contain one model\n",
    "        df2 should contain the other models to test over\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14,7))\n",
    "    plot_no = 0\n",
    "    for datasets in datasetses:\n",
    "        for comparison in comparisons:\n",
    "            plot_no += 1\n",
    "            ax = fig.add_subplot(1, len(datasetses)*len(comparisons),plot_no)\n",
    "                \n",
    "            rank_name = comparison+'_rank'\n",
    "            median_name = comparison+'_rank'\n",
    "            # filter to selected datasets\n",
    "            df_filt = df[df.target.isin(datasets)]\n",
    "            # get ranks\n",
    "            ascending = True if comparison == 'size' else False\n",
    "            df_filt[rank_name] = df_filt.groupby(['target','RunID'])[comparison].rank(ascending=ascending)\n",
    "#             df_filt[median_name] = df_filt.groupby(['target','RunID'])[comparison].median().reset_index()\n",
    "\n",
    "            df_median = df_filt.groupby(['model_nice','target','RunID']).median().reset_index()\n",
    "#             print('df_median:',df_median)\n",
    "\n",
    "            # get FEAT ranks\n",
    "            rank1 = df_filt[df_filt.model_nice == 'FEAT'][rank_name]\n",
    "            score1 = df_median[df_median.model_nice == 'FEAT'][comparison]\n",
    "\n",
    "            df2 = df_filt[df_filt.model_nice != 'FEAT']\n",
    "            p = {}\n",
    "            test_count = 0\n",
    "            for model,dfm2 in df2.groupby('model_nice'):\n",
    "                rank2 = dfm2[rank_name]\n",
    "                score2 = df_median[df_median.model_nice == model][comparison]\n",
    "#                 score2 = dfm2[median_name]\n",
    "                #Wilcoxon rank sum test\n",
    "#                 print(rank1,rank2)\n",
    "                _, p[('FEAT', model)] = wilcoxon(rank1, rank2, alternative='two-sided')\n",
    "                test_count += 1\n",
    "                # print absolute differences\n",
    "                print(datasets,'difference in',comparison,', Feat vs',model,':',\n",
    "                      (score1.mean()-score2.mean())/(score1.mean())\n",
    "                     )\n",
    "            \n",
    "            # bonferroni correction for multiple comparisons\n",
    "        #     p_adjusted = {}\n",
    "        #     for k,v in p.items():\n",
    "        #         p_adjusted[k] = v * test_count\n",
    "        #         print(k,p_adjusted[k],'*' if p_adjusted[k] < 0.05 else '')\n",
    "            p_adjusted = p\n",
    "\n",
    "\n",
    "            order = ['GNB','DT','LR L2','LR L1','FEAT','RF']\n",
    "            g = sns.boxplot(data=df_filt, x='model_nice',y=comparison,\n",
    "                            order=order,\n",
    "                            fliersize=False,\n",
    "#                             notch=True,\n",
    "#                             edgecolor=(0,0,0),\n",
    "#                             linewidth=2,\n",
    "                            ax = ax,\n",
    "        #                     fill=False\n",
    "                            color='w'\n",
    "                           )\n",
    "            # make box edges black\n",
    "            for j, box in enumerate(ax.artists):\n",
    "                box.set_edgecolor('black')\n",
    "                # iterate over whiskers and median lines\n",
    "                for k in range(6*j,6*(j+1)):\n",
    "                     ax.lines[k].set_color('black') \n",
    "            plt.xlabel('')\n",
    "            plt.ylabel(comparison.replace('_',' ').title())\n",
    "            if comparison=='size':\n",
    "                ax.set_yscale('log')\n",
    "            if len(datasets) == 3 and all(['Heuri' in d for d in datasets]):\n",
    "                plt.title('Heuristics')\n",
    "            elif len(datasets) == 3 and all(['dx' in d for d in datasets]):\n",
    "                plt.title('Chart Review')\n",
    "            else:\n",
    "                plt.title(','.join(datasets))\n",
    "            alpha = 0.001\n",
    "            pvalues = { bp: pval for bp, pval in p_adjusted.items() if pval < alpha }\n",
    "            \n",
    "#             box_pairs, pvalues = zip(*pvalues.items())\n",
    "            box_pairs, pvalues = zip(*p_adjusted.items())\n",
    "            \n",
    "            add_stat_annotation(g, data=df_median, x='model_nice', y=comparison,\n",
    "                                box_pairs=box_pairs, \n",
    "                                pvalues=pvalues, \n",
    "                                perform_stat_test=False,\n",
    "                                order=order, \n",
    "                                text_format='simple',\n",
    "                                show_test_name=False,\n",
    "                                pvalue_thresholds = [\n",
    "                                                     [1e-6, \"1e-6 ****\"], \n",
    "                                                     [1e-5, \"1e-5 ***\"], \n",
    "                                                     [1e-4, \"1e-4 **\"],\n",
    "                                                     [1e-3, \"1e-3 *\"], \n",
    "                                                     [0.01, \"1e-2 \"], \n",
    "                                                     [0.1, \"1e-1\"], \n",
    "                                                     [1, \"1 (ns)\"],\n",
    "                                                    ],\n",
    "                               )\n",
    "    if not os.path.exists('figs/'+rdir):\n",
    "        os.mkdir('figs/'+rdir)\n",
    "    for filetype in ['.png','.pdf','.eps','.svg']:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figs/'+rdir+'/rankings_boxplot_'+'-'.join(comparisons)\n",
    "                    +'_'.join(['-'.join(d) for d in datasetses])+filetype,\n",
    "                    dpi=400)\n",
    "\n",
    "    return p_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical test for AUPRCs modeling Heuristics (Wilcoxon rank-sum)\n",
    "from scipy.stats import wilcoxon\n",
    "from statannot import add_stat_annotation\n",
    "import pdb\n",
    "sns.set_style('whitegrid')\n",
    "def stats_test_bar(df, datasetses, comparisons):\n",
    "    \"\"\"Compare results from df1 and df2 over models on datasets according to comparison.\n",
    "        df1 should contain one model\n",
    "        df2 should contain the other models to test over\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    plot_no = 0\n",
    "    for datasets in datasetses:\n",
    "        for comparison in comparisons:\n",
    "            plot_no += 1\n",
    "            ax = fig.add_subplot(len(comparisons),len(datasetses),plot_no)\n",
    "                \n",
    "            rank_name = comparison+'_rank'\n",
    "            median_name = comparison+'_rank'\n",
    "            # filter to selected datasets\n",
    "            df_filt = df[df.target.isin(datasets)]\n",
    "            # get ranks\n",
    "            ascending = True if comparison == 'size' else False\n",
    "            df_filt[rank_name] = df_filt.groupby(['target','RunID'])[comparison].rank(ascending=ascending)\n",
    "#             df_filt[median_name] = df_filt.groupby(['target','RunID'])[comparison].median().reset_index()\n",
    "\n",
    "            df_median = df_filt.groupby(['model_nice','target','RunID']).median().reset_index()\n",
    "#             print('df_median:',df_median)\n",
    "\n",
    "            # get FEAT ranks\n",
    "            rank1 = df_filt[df_filt.model_nice == 'FEAT'][rank_name]\n",
    "            score1 = df_median[df_median.model_nice == 'FEAT'][comparison]\n",
    "\n",
    "            df2 = df_filt[df_filt.model_nice != 'FEAT']\n",
    "            p = {}\n",
    "            test_count = 0\n",
    "            for model,dfm2 in df2.groupby('model_nice'):\n",
    "                rank2 = dfm2[rank_name]\n",
    "                score2 = df_median[df_median.model_nice == model][comparison]\n",
    "#                 score2 = dfm2[median_name]\n",
    "                #Wilcoxon rank sum test\n",
    "#                 print(rank1,rank2)\n",
    "                _, p[('FEAT', model)] = wilcoxon(rank1, rank2, alternative='two-sided')\n",
    "                test_count += 1\n",
    "                # print absolute differences\n",
    "                print(datasets,'difference in',comparison,', Feat vs',model,':',\n",
    "                      (score1.mean()-score2.mean())/(score1.mean())\n",
    "                     )\n",
    "            \n",
    "            # bonferroni correction for multiple comparisons\n",
    "        #     p_adjusted = {}\n",
    "        #     for k,v in p.items():\n",
    "        #         p_adjusted[k] = v * test_count\n",
    "        #         print(k,p_adjusted[k],'*' if p_adjusted[k] < 0.05 else '')\n",
    "            p_adjusted = p\n",
    "\n",
    "\n",
    "            order = ['GNB','DT','LR L2','LR L1','FEAT','RF']\n",
    "            g = sns.barplot(data=df_filt, x='model_nice',y=comparison,\n",
    "                            order=order,\n",
    "                            edgecolor=(0,0,0),\n",
    "#                             linewidth=2,\n",
    "                            ax = ax,\n",
    "        #                     fill=False\n",
    "                            color='w'\n",
    "                           )\n",
    "            # make box edges black\n",
    "#             for j, box in enumerate(ax.artists):\n",
    "#                 box.set_edgecolor('black')\n",
    "#                 # iterate over whiskers and median lines\n",
    "#                 for k in range(6*j,6*(j+1)):\n",
    "#                      ax.lines[k].set_color('black') \n",
    "            plt.xlabel('')\n",
    "            if 'precision' in comparison.lower():\n",
    "                plt.ylabel('AUPRC, Test')\n",
    "            else:\n",
    "                plt.ylabel(comparison.replace('_',' ').title())\n",
    "            if comparison=='size':\n",
    "                ax.set_yscale('log')\n",
    "            if len(datasets) == 3 and all(['Heuri' in d for d in datasets]):\n",
    "                plt.title('Heuristics')\n",
    "            elif len(datasets) == 3 and all(['dx' in d for d in datasets]):\n",
    "                plt.title('Chart Review')\n",
    "            else:\n",
    "                plt.title(','.join(datasets))\n",
    "            alpha = 0.001\n",
    "            pvalues = { bp: pval for bp, pval in p_adjusted.items() if pval < alpha }\n",
    "            \n",
    "#             box_pairs, pvalues = zip(*pvalues.items())\n",
    "            box_pairs, pvalues = zip(*p_adjusted.items())\n",
    "            \n",
    "            add_stat_annotation(g, data=df_median, x='model_nice', y=comparison,\n",
    "                                box_pairs=box_pairs, \n",
    "                                pvalues=pvalues, \n",
    "                                perform_stat_test=False,\n",
    "                                order=order, \n",
    "                                text_format='simple',\n",
    "                                show_test_name=False,\n",
    "                                pvalue_thresholds = [\n",
    "                                                     [1e-6, \"1e-6 ****\"], \n",
    "                                                     [1e-5, \"1e-5 ***\"], \n",
    "                                                     [1e-4, \"1e-4 **\"],\n",
    "                                                     [1e-3, \"1e-3 *\"], \n",
    "                                                     [0.01, \"1e-2 \"], \n",
    "                                                     [0.1, \"1e-1\"], \n",
    "                                                     [1, \"1 (ns)\"],\n",
    "                                                    ],\n",
    "                               )\n",
    "    if not os.path.exists('figs/'+rdir):\n",
    "        os.mkdir('figs/'+rdir)\n",
    "    for filetype in ['.png','.pdf','.eps','.svg']:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('figs/'+rdir+'/rankings_barplot_'+'-'.join(comparisons)\n",
    "                    +'_'.join(['-'.join(d) for d in datasetses])+filetype,\n",
    "                    dpi=400)\n",
    "\n",
    "    return p_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "heuristics =['HtnHeuri','HtnHypoKHeuri',\"ResHtnHeuri\"]\n",
    "diagnoses = ['Htndx',\"HtnHypoKdx\",\"ResHtndx\"]\n",
    "datasetses = [heuristics, diagnoses]\n",
    "# datasetses = [[[h] for h in heuristics] + [[d] for d in diagnoses]]\n",
    "# datasetses = [heuristics[:] + diagnoses[:],\n",
    "#               heuristics, diagnoses,\n",
    "#               ] + [[h] for h in heuristics] + [[d] for d in diagnoses]\n",
    "comparisons = ['average_precision_score_test', 'size']\n",
    "pvalues = []\n",
    "df_feat_ave = df_results_ave.loc[df_results_ave.model_nice == 'FEAT']\n",
    "df_other_ave = df_results_ave.loc[df_results_ave.model_nice != 'FEAT']\n",
    "# for datasets in datasetses:\n",
    "#     for comparison in comparisons:\n",
    "print(datasetses, comparisons)\n",
    "result = stats_test_bar(df_results_ave, datasetses, comparisons)\n",
    "\n",
    "stats_test_box(df_results_ave, [[d] for d in diagnoses], comparisons)\n",
    "# pdict['Comparison'] = comparison\n",
    "# pdict['Datasets'] = ', '.join(datasets)\n",
    "# pdict.update(result)\n",
    "# pvalues.append(pdict)\n",
    "# print(p_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
