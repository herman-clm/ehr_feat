{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook 1) aggregates and averages the five-fold results and 2) implements the model selection procedure based on the repeated cross-validation results. Final outputs are saved as \"select_perf.csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#embed fonts\n",
    "import matplotlib\n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "import scipy.stats\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dnames = [\n",
    "    'HtnHeuri','HtnHypoKHeuri','ResHtnHeuri',\n",
    "    'Htndx','HtnHypoKdx','ResHtndx',\n",
    "]\n",
    "dnames_nice = [\n",
    "               'HTN Heuristic','HTN-Hypokalemia Heuristic',\"Resistant HTN Heuristic\",\n",
    "               'HTN Diagnosis',\"HTN-Hypokalemia Diagnosis\",\"Resistant HTN Diagnosis\",\n",
    "]\n",
    "dnames_to_nice = {k:v for k,v in zip(dnames, dnames_nice)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdir = 'resultsFinal_r1'\n",
    "frames = []\n",
    "feat_frames = []\n",
    "#Read benchmark model results\n",
    "for file in tqdm(glob.glob('../'+rdir+'/*/*/*.json')):\n",
    "    print(file)\n",
    "    with open(file,'r') as of:\n",
    "        results = json.load(of)\n",
    "    if 'Feat' in file:\n",
    "        feat_frames.append(results)\n",
    "    else:\n",
    "        frames.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_records(frames)\n",
    "feat_df_results = pd.DataFrame.from_records(feat_frames)\n",
    "for df in [df_results, feat_df_results]:\n",
    "    print('results columns:',df.columns)    \n",
    "    print('models:',df.model.unique())\n",
    "    print('targets:',df.target.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manually add heuristic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "dx_to_heu = {k:v for k,v in zip(dnames[3:],dnames[:3])}\n",
    "targets = {\n",
    "            'htn_dx_ia':'Htndx',\n",
    "            'res_htn_dx_ia':'ResHtndx', \n",
    "            'htn_hypok_dx_ia':'HtnHypoKdx', \n",
    "            'HTN_heuristic':'HtnHeuri', \n",
    "            'res_HTN_heuristic':'ResHtnHeuri',\n",
    "            'hypoK_heuristic_v4':'HtnHypoKHeuri'\n",
    "            }\n",
    "rev_targets = {v:k for k,v in targets.items()}\n",
    "drop_cols = ['UNI_ID'] + list(targets.keys())\n",
    "\n",
    "print(dx_to_heu)\n",
    "frames = []\n",
    "for target in df_results.target.unique():\n",
    "    if target in dx_to_heu.keys():\n",
    "        heuristic = dx_to_heu[target]\n",
    "        print('target:',target,'heuristic:',heuristic)\n",
    "        target_raw = rev_targets[target]\n",
    "        df_train = pd.read_csv('../Dataset' + str(101) + '/' + target + '/' + target + 'ATrain.csv')\n",
    "        y_train = df_train[target_raw].values\n",
    "        df_X_train = df_train.drop(drop_cols,axis=1)  \n",
    "        df_test = pd.read_csv( '../Dataset' + str(101) + '/' + target + '/' + target + 'ATest.csv')\n",
    "        df_X_test = df_test.drop(drop_cols,axis=1)  \n",
    "        y_test = df_test[target_raw].values\n",
    "        \n",
    "        print(\n",
    "            'phenotype cases:',np.sum(y_test==1)+np.sum(y_train==1),\n",
    "            'phenotype controls:',np.sum(y_test==0)+np.sum(y_train==0),\n",
    "              'out of',len(y_test)+len(y_train))\n",
    "        y_heu = df_test[rev_targets[heuristic]]\n",
    "        \n",
    "        frames.append({'model':'Heuristic',\n",
    "                       'target':target,\n",
    "                       'RunID':101,\n",
    "                       'average_precision_score_test': average_precision_score(y_test, y_heu),\n",
    "                       'precision': np.sum((y_heu==1) & (y_test == 1))/ np.sum(y_heu==1),\n",
    "                       'recall': np.sum((y_heu==1) & (y_test == 1))/ np.sum(y_test==1),\n",
    "                       'specificity': np.sum((y_heu==0) & (y_test == 0))/ np.sum(y_test==0),\n",
    "                       'roc_auc_score_test': roc_auc_score(y_test, y_heu)\n",
    "                      })\n",
    "df_heu = pd.DataFrame.from_records(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.append(df_heu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, dfg in feat_df_results.groupby('target'):\n",
    "    print('===============')\n",
    "    print('target:',target)\n",
    "    print('===============')\n",
    "    for r in dfg['representation'].values:\n",
    "        print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## down-select FEAT models from runs using heuristic procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection import select_feat_models, smallest_of_best_three_quartiles\n",
    "import pandas as pd\n",
    "\n",
    "feat_df_results_reduced = select_feat_models(feat_df_results, method= smallest_of_best_three_quartiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feat_df_results_reduced)\n",
    "# feat_df_results.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dataframes\n",
    "df_results = df_results.append(feat_df_results_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.isna().any()\n",
    "df_results.model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results.model=='Feat_boolean_L1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make nice labels\n",
    "models = ['RandomForest',\n",
    "          'DecisionTree',\n",
    "          'Feat_boolean',\n",
    "          'Feat_boolean_L1',\n",
    "          'GaussianNaiveBayes',\n",
    "          'LogisticRegressionCV_L2',\n",
    "          'LogisticRegressionCV_L1',\n",
    "          'Heuristic']\n",
    "model_nice = ['RF',\n",
    "          'DT',\n",
    "          'FEAT',\n",
    "          'FEAT L1',\n",
    "          'GNB',\n",
    "          'LR L2',\n",
    "          'LR L1',\n",
    "            'Heuristic'\n",
    "             ]\n",
    "nice_model_labels = {k:v for k,v in zip(models,model_nice)}\n",
    "df_results['model_nice'] = df_results['model'].apply(lambda x: nice_model_labels[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save output to select_perf.csv\n",
    "import os\n",
    "if not os.path.exists(rdir):\n",
    "    os.mkdir(rdir)\n",
    "df_results.to_csv(rdir + '/select_perf_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average values by run (average all folds)\n",
    "df_results['size'] = df_results['size'].astype(float)\n",
    "df_results_ave = df_results.groupby(['model_nice','target','RunID'], as_index=False).mean()\n",
    "# df_results_ave = df_results.groupby(['model_nice','target','RunID','selection'], as_index=False).mean()\n",
    "df_results_ave.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_ave.model_nice.unique()\n",
    "df_results_ave[df_results_ave.model_nice=='FEAT'].target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make docx table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "\n",
    "document = Document()\n",
    "\n",
    "table = document.add_table(rows=1, cols=5)\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'Phenotype'\n",
    "hdr_cells[1].text = 'Method'\n",
    "hdr_cells[2].text = 'Test AUPRC'\n",
    "hdr_cells[3].text = 'Test AUROC'\n",
    "hdr_cells[4].text = 'Size'\n",
    "# for target, dft in df_results_ave.groupby('target'):\n",
    "i = 0\n",
    "for target in dnames:\n",
    "    print(target)\n",
    "    dft = df_results_ave.loc[df_results_ave.target == target]\n",
    "    j = 0\n",
    "    models = ['GNB','DT','LR L1','LR L2','RF','FEAT']\n",
    "    if 'dx' in target:\n",
    "        models.append('Heuristic')\n",
    "    for model in models:\n",
    "        dftm = dft.loc[dft.model_nice == model]\n",
    "        i += 1\n",
    "        j += 1\n",
    "        table.add_row()\n",
    "        cells = table.rows[i].cells\n",
    "        if j == 4:\n",
    "            cells[0].text = dnames_to_nice[target]\n",
    "        assert (len(dftm) == 1)\n",
    "        cells[1].text = model\n",
    "        cells[2].text = '{:0.2f}'.format( dftm['average_precision_score_test'].values[0])\n",
    "        cells[3].text = '{:0.2f}'.format( dftm['roc_auc_score_test'].values[0])\n",
    "        if model != 'Heuristic':\n",
    "            cells[4].text = '{:d}'.format( int(dftm['size'].values[0]))\n",
    "        else:\n",
    "            cells[4].text = '-'\n",
    "        print('-','\\t\\t'.join([c.text for c in cells[1:]]))\n",
    "    print(50*'=') \n",
    "# for qty, id, desc in records:\n",
    "#     row_cells = table.add_row().cells\n",
    "#     row_cells[0].text = str(qty)\n",
    "#     row_cells[1].text = id\n",
    "#     row_cells[2].text = desc\n",
    "\n",
    "document.add_page_break()\n",
    "\n",
    "document.save('tables/Table_Final_Models.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
