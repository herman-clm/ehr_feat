{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates precision recall curves and receiver operating curves from the repeated five-fold cross validation runs (Figure 3). You must have generated 'select_perf.csv' from the notebook \"Model Selection + Statistical Tests + Data Visualization.ipynb.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the max precision the final point in the graph, not 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (roc_auc_score,\n",
    "                             roc_curve,\n",
    "                             precision_recall_curve,\n",
    "                             auc,\n",
    "                             average_precision_score\n",
    "                             )\n",
    "from scipy import interp\n",
    "from scipy.stats import sem, t\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Read select_perf.csv\n",
    "rdir = 'results_2020-09-14'\n",
    "perf = pd.read_csv(rdir + '/select_perf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb \n",
    "\n",
    "def parse(pred):\n",
    "    pred = pred.replace('\\n','')\n",
    "    pred = pred.replace('[','')\n",
    "    pred = pred.replace(']','')\n",
    "    pred = list(map(float,pred.split()))\n",
    "    return pred\n",
    "\n",
    "def prc_values(y,y_pred_proba):\n",
    "    precision, recall, prcthresholds = precision_recall_curve(y, y_pred_proba, pos_label=1)\n",
    "    precision[-1] = np.max(precision[:-1])\n",
    "    s = np.argsort(recall)\n",
    "    precision = precision[s]\n",
    "    recall = recall[s]\n",
    "    mean_recall = np.linspace(0.0, 1, 21)\n",
    "    precision = interp(mean_recall, recall, precision)\n",
    "    return mean_recall, precision\n",
    "\n",
    "def roc_values(y,y_pred_proba):\n",
    "    fpr,tpr, rocthresholds = roc_curve(y, y_pred_proba, pos_label=1)\n",
    "    roc = pd.DataFrame(list(zip(fpr,tpr, rocthresholds)), columns =['fpr','tpr','thresholds']) \n",
    "    roc = roc.sort_values(by='fpr')\n",
    "    tpr = roc['tpr']\n",
    "    fpr = roc['fpr']\n",
    "    mean_fpr = np.linspace(0, 1, 21)\n",
    "    tpr = interp(mean_fpr, fpr, tpr)\n",
    "    return mean_fpr, tpr\n",
    "#Import correct labels\n",
    "targets = {\n",
    "            'htn_dx_ia':'Htndx',\n",
    "            'res_htn_dx_ia':'ResHtndx', \n",
    "            'htn_hypok_dx_ia':'HtnHypoKdx', \n",
    "            'HTN_heuristic':'HtnHeuri', \n",
    "            'res_HTN_heuristic':'ResHtnHeuri',\n",
    "            'hypoK_heuristic_v4':'HtnHypoKHeuri'\n",
    "            }\n",
    "heuristics = {\n",
    "            'Htndx':'HTN_heuristic',\n",
    "            'ResHtndx':'res_HTN_heuristic',\n",
    "            'HtnHypoKdx':'hypoK_heuristic_v4'\n",
    "    \n",
    "}\n",
    "targets_rev = {v:k for k,v in targets.items()}\n",
    "dnames = ['Htndx',\"HtnHypoKdx\",\"ResHtndx\",'HtnHeuri','HtnHypoKHeuri',\"ResHtnHeuri\"]\n",
    "dnames_nice = ['HTN Diagnosis',\"HTN-Hypokalemia Diagnosis\",\"Resistant HTN Diagnosis\",\n",
    "               'HTN Heuristic','Htn-Hypokalemia Heuristic',\"Resistant HTN Heuristic\"]\n",
    "dnames_to_nice = {k:v for k,v in zip(dnames, dnames_nice)}\n",
    "\n",
    "folds = ['A','B','C','D','E']\n",
    "\n",
    "models = ['RandomForest',\n",
    "          'DecisionTree',\n",
    "          'Feat_boolean',\n",
    "          'Feat_boolean_L1',\n",
    "          'GaussianNaiveBayes',\n",
    "          'LogisticRegression_L2',\n",
    "          'LogisticRegression_L1']\n",
    "model_nice = ['RF',\n",
    "          'DT',\n",
    "          'FEAT',\n",
    "          'FEAT L1',\n",
    "          'GNB',\n",
    "          'LR L2',\n",
    "          'LR L1']\n",
    "nice_model_labels = {k:v for k,v in zip(models,model_nice)}\n",
    "nice_to_ugly = {v:k for k,v in nice_model_labels.items()}\n",
    "markers = ('^','o', 's', 'p', 'h', 'D', 'P', 'X', '*','v', '<', '>',)\n",
    "order = ['GNB','DT', 'LR L2','LR L1','RF','FEAT']\n",
    "marker_choice = {\n",
    "    'GNB':'s',\n",
    "    'DT':'o',\n",
    "    'LR L2':'d',\n",
    "    'LR L1':'P',\n",
    "    'RF':(4,1,0), #'D',\n",
    "    'FEAT':'X'\n",
    "}\n",
    "\n",
    "\n",
    "spacing = 5\n",
    "fontsize=14\n",
    "# fig = plt.figure(figsize=(12,6))\n",
    "for target_new, perf_t in perf.groupby('target'):\n",
    "    if target_new != 'ResHtndx': continue\n",
    "#     ax = fig.add_subplot(2,3, i) \n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    # axis for PR curve\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    # axis for ROC curve\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    \n",
    "    i = 1\n",
    "#     for m, (model, perf_t_m) in enumerate(perf_t.groupby('model')):\n",
    "    for m, model_nice in enumerate(order):\n",
    "        model = nice_to_ugly[model_nice]\n",
    "        perf_t_m = perf_t.loc[perf_t.model==model] \n",
    "        if model == 'Feat_boolean_L1': \n",
    "            continue\n",
    "        print('graphing ',target_new,model)\n",
    "        mean_run_precisions = []\n",
    "        mean_run_tprs = []\n",
    "        if i == 1 and target_new in heuristics.keys():\n",
    "            mean_run_precision_h = []\n",
    "            mean_run_recall_h = []\n",
    "            mean_run_fpr_h = []\n",
    "            mean_run_tpr_h = []\n",
    "        for RunID, perf_t_m_id in perf_t_m.groupby('RunID'):\n",
    "            precisions = []\n",
    "            tprs = []\n",
    "            precisions_h = []\n",
    "            recalls_h = []\n",
    "            fprs_h = []\n",
    "            tprs_h = []\n",
    "            for fold, perf_t_m_id_f in perf_t_m_id.groupby('fold'):\n",
    "\n",
    "                #True labels\n",
    "                df = pd.read_csv('../Dataset' + str(RunID) + '/' + target_new + '/' + target_new + fold \n",
    "                                 + 'Test.csv')\n",
    "                y = df[targets_rev[target_new]].values\n",
    "                \n",
    "                # handle the heuristic\n",
    "                if i == 1 and target_new in heuristics.keys():\n",
    "                    y_heuristic = df[heuristics[target_new]].values\n",
    "#                     print('y_heuristic:',y_heuristic)\n",
    "                    precision_h = np.sum((y==1) & (y_heuristic==1))/np.sum(y_heuristic==1)\n",
    "                    recall_h = np.sum((y==1) & (y_heuristic==1))/np.sum(y==1)\n",
    "#                     print('precision_h:',precision_h)\n",
    "#                     print('recall_h:',recall_h)\n",
    "#                     precision_h, recall_h = precision_recall_curve(y, y_heuristic)\n",
    "                    precisions_h.append(precision_h)\n",
    "                    recalls_h.append(recall_h)\n",
    "#                     fpr_h, tpr_h = roc_curve(y, y_heuristic)\n",
    "                    fpr_h = np.sum((y==0) & (y_heuristic==1))/np.sum(y==0) \n",
    "                    tpr_h = recall_h\n",
    "#                     print('fpr_h:',fpr_h)\n",
    "#                     print('tpr_h:',tpr_h)\n",
    "                \n",
    "                    fprs_h.append(fpr_h)\n",
    "                    tprs_h.append(tpr_h)\n",
    "                    heuristic=False\n",
    "#                 print('y:',len(y))\n",
    "                #Predicted probabilities\n",
    "                assert(len(perf_t_m_id_f)==1)\n",
    "                y_pred_proba = eval(perf_t_m_id_f['pred_proba'].values[0])\n",
    "            \n",
    "                # Precision / Recall\n",
    "                ####################\n",
    "                mean_recall, precision = prc_values(y,y_pred_proba)\n",
    "                precisions.append(precision)\n",
    "            \n",
    "                # ROC\n",
    "                #####\n",
    "                mean_fpr, tpr = roc_values(y,y_pred_proba)\n",
    "                tprs.append(tpr)\n",
    "            \n",
    "            #mean_run_precisions: The mean of five fold precisions\n",
    "            mean_run_precisions.append(np.mean(precisions, axis=0))\n",
    "            #mean_run_tprs: The mean of five fold tprs\n",
    "            mean_run_tprs.append(np.mean(tprs, axis=0))\n",
    "            if i == 1 and target_new in heuristics.keys():\n",
    "                mean_run_precision_h.append(np.mean(precisions_h, axis=0))\n",
    "                mean_run_recall_h.append(np.mean(recalls_h, axis=0))\n",
    "                mean_run_fpr_h.append(np.mean(fprs_h, axis=0))\n",
    "                mean_run_tpr_h.append(np.mean(tprs_h, axis=0))\n",
    "            \n",
    "        #mean_precisions: The mean of mean_run_precisions over 50 iterations\n",
    "        mean_precisions = np.mean(mean_run_precisions, axis=0)\n",
    "        #mean_tprs: The mean of mean_run_tprs over 50 iterations\n",
    "        mean_tprs = np.mean(mean_run_tprs, axis=0)\n",
    "        \n",
    "#         plt.figure(target_new, figsize=(10, 6))\n",
    "        # Precision/Recall plot \n",
    "        ax1.plot(mean_recall, mean_precisions, \n",
    "                 alpha=1,\n",
    "                 label=nice_model_labels[model],\n",
    "                 marker = marker_choice[model_nice], \n",
    "                 markevery=spacing)\n",
    "        if model == 'Feat_boolean':\n",
    "            #print(mean_run_precisions)\n",
    "            std_err = sem(mean_run_precisions, axis=0)\n",
    "            h = std_err * t.ppf(1.95/2, len(mean_run_precisions) - 1)\n",
    "            precisions_upper = np.minimum(mean_precisions + h, 1)\n",
    "            precisions_lower = np.maximum(mean_precisions - h, 0)\n",
    "            ax1.fill_between(mean_recall, precisions_lower, precisions_upper, \n",
    "                             color='grey', alpha=.2, label=r'95% Confidence Interval')\n",
    "        # ROC plot\n",
    "        #####\n",
    "        ax2.plot(mean_fpr, mean_tprs, \n",
    "                 alpha=1,\n",
    "                 label=nice_model_labels[model],\n",
    "#                  marker = markers[m], \n",
    "                 marker = marker_choice[model_nice], \n",
    "                 markevery=spacing)\n",
    "        ax2.plot([0,1],[0,1],'--k',label=None)\n",
    "        if model == 'Feat_boolean':\n",
    "            #print(mean_run_tprs)\n",
    "            std_err = sem(mean_run_tprs, axis=0)\n",
    "            h = std_err * t.ppf(1.95/2, len(mean_run_tprs) - 1)\n",
    "            tprs_upper = np.minimum(mean_tprs + h, 1)\n",
    "            tprs_lower = np.maximum(mean_tprs - h, 0)\n",
    "            ax2.fill_between(mean_fpr, tprs_lower, tprs_upper, \n",
    "                             color='grey', alpha=.2, label=r'95% Confidence Interval')\n",
    "        i+=1\n",
    "            \n",
    "    # heuristic performance\n",
    "#     print('mean_run_precision_h:',mean_run_precision_h)\n",
    "#     print('mean_run_recall_h:',mean_run_recall_h)\n",
    "#     print('mean_run_fpr_h:',mean_run_fpr_h)\n",
    "#     print('mean_run_tpr_h:',mean_run_tpr_h)\n",
    "    mean_recall_h = np.mean(mean_run_recall_h, axis=0)\n",
    "    mean_precision_h = np.mean(mean_run_precision_h, axis=0)\n",
    "    mean_fpr_h = np.mean(mean_run_fpr_h, axis=0)\n",
    "    mean_tpr_h = np.mean(mean_run_tpr_h, axis=0)\n",
    "    print(mean_recall_h, mean_precision_h, mean_fpr_h, mean_tpr_h)\n",
    "    # plot heuristic\n",
    "    ax1.plot(mean_recall_h,\n",
    "             mean_precision_h,\n",
    "             'Xk',\n",
    "             label='Heuristic',\n",
    "            ) \n",
    "    # plot heuristic\n",
    "    ax2.plot(mean_fpr_h,\n",
    "             mean_tpr_h,\n",
    "             'Xk',\n",
    "             label='Heuristic',\n",
    "            ) \n",
    "    plt.suptitle(dnames_to_nice[target_new], fontsize=fontsize)\n",
    "    ax1.set_xlabel(\"Recall (Sensitivity)\", fontsize=fontsize)\n",
    "    ax1.set_ylabel(\"Precision\", fontsize=fontsize)\n",
    "    ax2.set_xlabel(\"1 - Specificity\", fontsize=fontsize)\n",
    "    ax2.set_ylabel(\"Sensitivity\", fontsize=fontsize)\n",
    "#     if i in [1,4]:\n",
    "#         plt.xlabel(\"Recall (Sensitivity)\")\n",
    "#     if i > 3:\n",
    "#         plt.ylabel(\"Precision (PPV)\")\n",
    "#     else:\n",
    "#         plt.xticks([])\n",
    "#     if i == 6:        \n",
    "#         plt.legend()\n",
    "    \n",
    "#     i += 1\n",
    "    plt.legend(loc='best')\n",
    "#     plt.tight_layout()\n",
    "    for filetype in ['.svg','.png','.pdf']:\n",
    "        plt.savefig('figs/'+rdir + '/' + target_new + '_PRC_ROC'+ filetype, dpi=400)\n",
    "\n",
    "# plt.show()        \n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
